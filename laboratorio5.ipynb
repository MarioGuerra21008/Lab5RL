{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e4f90f",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier√≠a - Computaci√≥n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci√≥n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 5:</strong> Time Difference Learning</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern√°ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In√©s Jim√©nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5ad97",
   "metadata": {},
   "source": [
    "## üìù Task 1\n",
    "\n",
    "**1. Defina y explique qu√© ‚Äúexpected sarsa‚Äù**\n",
    "**a. ¬øC√≥mo se diferencia de ‚Äúsarsa‚Äù?**\n",
    "**b. ¬øPara qu√© sirven las modificaciones que se hacen sobre ‚Äúsarsa‚Äù?**\n",
    "\n",
    "- Expected SARSA es una variante del algoritmo original en el que, en vez de actualizar el valor Q usando solamente la acci√≥n que se tom√≥ en el siguiente estado, este utiliza el valor esperado de todas las acciones posibles de ese estado, siendo ponderado por la probabilidad de que cada una sea elegida seg√∫n la pol√≠tica actual. Su diferencia con sarsa se da en que, en lugar de tomar Q(s', a'), toma la sumatoria de todos los valores Q de todas las acciones en s'. Su utilidad en las modificaciones se da en reducir la varianza de la estimaci√≥n por cada iteraci√≥n, suavizando el aprendizaje.\n",
    "\n",
    "**2. Defina y explique qu√© es ‚Äún-step TD‚Äù**\n",
    "**a. ¬øC√≥mo se diferencia de TD(0)?**\n",
    "**b. ¬øCu√°l es la utilidad de esta modificaci√≥n?**\n",
    "**c. ¬øQu√© usa como objetivo?**\n",
    "\n",
    "- Es un m√©todo de Temporal Difference que actualiza los valores Q utilizando el retorno acumulado de n-pasos en el futuro, en vez de hacerlo solo con uno. La diferencia de TD(0) es que n-step TD utiliza la suma de recompensas de los siguientes n-pasos m√°s el valor estimado en el estado alcanzado. Su utilidad permite un balance entre el sesgo que se puede generar y la varianza, considerando los valores peque√±os y grandes de n. El objetivo que usa es el propio retorno de n-step.\n",
    "\n",
    "\n",
    "\n",
    "**3. ¬øCu√°l es la diferencia entre SARSA y Q-learning?**\n",
    "\n",
    "- SARSA es un algoritmo de naturaleza on-policy que aprende el valor Q siguiendo la pol√≠tica mientras esta se est√° evaluando y mejorando. Mientras que, Q-Learning, por otro lado, es de naturaleza off-policy, y este aprende el valor Q de la pol√≠tica √≥ptima independientemente de la que se siga para explorar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3f90e",
   "metadata": {},
   "source": [
    "## üìù Task 2\n",
    "\n",
    "En este laboratorio, comparar√°n el rendimiento de SARSA y Q-Learning, dos algoritmos de aprendizaje de refuerzo populares, utilizando el entorno CliffWalking-v0 de la biblioteca Gymnasium. Analizar√° y graficar√° las recompensas por episodio y responder√° preguntas para profundizar su comprensi√≥n de las diferencias entre estos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9210fc6",
   "metadata": {},
   "source": [
    "**Instalar las bibliotecas necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a0c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from gymnasium==0.28.1) (2.3.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from gymnasium==0.28.1) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from gymnasium==0.28.1) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from gymnasium==0.28.1) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mague\\downloads\\lab5rl\\.venv\\lib\\site-packages (from gymnasium==0.28.1) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install gymnasium==0.28.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d014a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.cliffwalking import CliffWalkingEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310f7a9",
   "metadata": {},
   "source": [
    "**Inicialice el entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c48b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno de CliffWalking-v0 | Estados: 48 | Acciones: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mague\\Downloads\\Lab5RL\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment CliffWalking-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"CliffWalking-v0\",\n",
    "    entry_point=\"gymnasium.envs.toy_text.cliffwalking:CliffWalkingEnv\",\n",
    ")\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Par√°metros por defecto\n",
    "ALPHA = 0.5         # tasa de aprendizaje\n",
    "GAMMA = 0.99        # factor de descuento\n",
    "EPSILON = 0.1       # √©psilon para pol√≠tica epsilon-greedy\n",
    "EPISODES = 500      # cantidad de episodios\n",
    "SEED = 24\n",
    "\n",
    "# Semillas\n",
    "np.random.seed(SEED)\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "print(f\"Entorno de CliffWalking-v0 | Estados: {n_states} | Acciones: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75146396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "def make_epsilon_schedule(epsilon_start, epsilon_end=None, decay_episodes=None):\n",
    "    if epsilon_end is None or decay_episodes is None or decay_episodes <= 0:\n",
    "        return lambda ep: epsilon_start\n",
    "    \n",
    "    def schedule(ep):\n",
    "        t = min(ep / decay_episodes, 1.0)\n",
    "        return (1 - t) * epsilon_start + t * epsilon_end\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49964f",
   "metadata": {},
   "source": [
    "**Implementar SARSA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdbe21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n",
    "                epsilon=EPSILON, epsilon_schedule=None, seed=SEED):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        eps = epsilon_schedule(ep) if epsilon_schedule else epsilon\n",
    "        state, info = env.reset(seed=seed + ep)\n",
    "        action = epsilon_greedy_action(Q, state, eps)\n",
    "\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Elegir siguiente acci√≥n con la misma pol√≠tica\n",
    "            next_action = epsilon_greedy_action(Q, next_state, eps)\n",
    "\n",
    "            # Actualizaci√≥n de SARSA\n",
    "            td_target = reward + gamma * Q[next_state, next_action] * (1 - int(terminated or truncated))\n",
    "            td_error  = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "    return rewards_per_episode, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22587c",
   "metadata": {},
   "source": [
    "**Implementar Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96303746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ec2bf0",
   "metadata": {},
   "source": [
    "**Ejecutar ambos algoritmos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2a5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef7f3ba7",
   "metadata": {},
   "source": [
    "**Gr√°fico de los resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1b217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e97bdce",
   "metadata": {},
   "source": [
    "**Analice los resultados**\n",
    "\n",
    "**a. Observe los resultados graficados y compare el rendimiento de SARSA y Q-Learning a lo largo de los episodios.**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**b. Considere c√≥mo cada algoritmo equilibra la exploraci√≥n y la explotaci√≥n.**\n",
    "\n",
    "- Respuesta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dee28",
   "metadata": {},
   "source": [
    "**Responda las siguientes preguntas:**\n",
    "\n",
    "**a. P1: ¬øQu√© diferencias observa en los patrones de recompensa entre SARSA y Q-Learning?**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**b. P2: ¬øPor qu√© Q-Learning tiende a ser m√°s optimista en sus actualizaciones de valor Q en comparaci√≥n con SARSA?**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**c. P3: ¬øC√≥mo afecta la naturaleza \"on-policy\" de SARSA a su proceso de aprendizaje en comparaci√≥n con la naturaleza \"off-policy\" de Q-Learning?**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**d. P4: Seg√∫n las penalizaciones y recompensas del entorno, ¬øqu√© algoritmo parece aprender el camino m√°s seguro? ¬øPor qu√©?**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**e. P5: ¬øC√≥mo podr√≠a afectar la disminuci√≥n de √©psilon con el tiempo al rendimiento de cada algoritmo?**\n",
    "\n",
    "- Respuesta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1208f39",
   "metadata": {},
   "source": [
    "**Preguntas para responder:**\n",
    "\n",
    "**1. ¬øCu√°l es el valor estimado de mantener diferentes niveles de existencias para cada producto?**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**2. ¬øC√≥mo afecta el valor epsilon en la pol√≠tica blanda al rendimiento?**\n",
    "\n",
    "- Respuesta\n",
    "\n",
    "**3. ¬øCu√°l es el impacto de utilizar el aprendizaje fuera de la pol√≠tica en comparaci√≥n con el aprendizaje dentro de la pol√≠tica?**\n",
    "\n",
    "- Respuesta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
